% author:   sam tenka
% change:   2022-05-11
% create:   2022-05-11

%==============================================================================
%=====  0.  DOCUMENT SETTINGS  ================================================
%==============================================================================

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  0.0. About and Beyond this Exposition  ~~~~~~~~~~~~~~~~~~~~~~~~

%---------------------  0.0.0. page geometry  ---------------------------------
\documentclass[11pt, justified]{tufte-book}
\geometry{
  left           = 0.90in, % left margin
  textwidth      = 4.95in, % main text block
  marginparsep   = 0.15in, % gutter between main text block and margin notes
  marginparwidth = 2.30in, % width of margin notes
                 % 0.20in  % width from margin to edge
}

%---------------------  0.0.1. math packages  ---------------------------------
\newcommand\hmmax{0}
\newcommand\bmmax{0}
\usepackage{amsmath, amssymb, amsthm, mathtools, bm, euler}
\usepackage{listings}

%---------------------  0.0.2. graphics packages  -----------------------------
\usepackage{graphicx, xcolor}
\usepackage{enumitem}\setlist{nosep}
\usepackage{float, capt-of}

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  0.1. Header Formatting  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

\definecolor{mgrn}{rgb}{0.15, 0.65, 0.05} \newcommand{\grn}{\color{mgrn}}
\definecolor{mred}{rgb}{0.90, 0.05, 0.05} \newcommand{\red}{\color{mred}}
\definecolor{mblu}{rgb}{0.05, 0.35, 0.70} \newcommand{\blu}{\color{mblu}}
\definecolor{mbre}{rgb}{0.30, 0.45, 0.60} \newcommand{\bre}{\color{mbre}}
\definecolor{mgre}{rgb}{0.55, 0.55, 0.50} \newcommand{\gre}{\color{mgre}}

%\newcommand{\note}[1]{{\blu \textsf{#1}}}
\newcommand{\attn}[1]{{\red \textsf{#1}}}

%---------------------  0.1.0. tidbit headers  --------------------------------
\newcommand{\samtitle} [1]{
  \par\noindent{\Huge \sf \blu #1}
  \vspace{0.4cm}
}

\newcommand{\samquote} [2]{
    \marginnote[-0.4cm]{\begin{flushright}
    \scriptsize
        \gre {\it #1} \\ --- #2
    \end{flushright}}
}

%---------------------  0.1.1. section headers  -------------------------------

\newcommand{\samsection} [1]{
  \vspace{0.5cm}
  \par\noindent{\LARGE \sf \blu #1}
  \vspace{0.1cm}\par
}

\newcommand{\samsubsection}[1]{
  \vspace{0.3cm}
  \par\noindent{\large \sf \bre #1}
  \vspace{0.1cm}\par
}

\newcommand{\samsubsubsection}[1]{
   \vspace{0.1cm}
   \par\noindent{\hspace{-2cm}\normalsize \sc \gre #1} ---
}

%---------------------  0.1.2. clear the bibliography's header  ---------------
\usepackage{etoolbox}
\patchcmd{\thebibliography}{\section*{\refname}}{}{}{}

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  0.2. Math Symbols and Blocks  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

%---------------------  0.2.0. probability symbols  ---------------------------
\newcommand{\KL}{\text{KL}}
\newcommand{\EN}{\text{H}}
\newcommand{\note}[1]{{\blu \textsf{#1}}}

\newcommand{\scirc}{\mathrel{\mathsmaller{\mathsmaller{\mathsmaller{\circ}}}}}
\newcommand{\cmop}[2]{{(#1\!\to\!#2)}}

% losses averaged in various ways: 
\newcommand{\Ein}  {\text{trn}_{\sS}}
\newcommand{\Einb} {\text{trn}_{\check\sS}}
\newcommand{\Einc} {\text{trn}_{\sS\sqcup \check\sS}}
\newcommand{\Egap} {\text{gap}_{\sS}}
\newcommand{\Eout} {\text{tst}}



%---------------------  0.2.1. double-struck and caligraphic letters  ---------
\newcommand{\Aa}{\mathbb{A}}\newcommand{\aA}{\mathcal{A}}
\newcommand{\Bb}{\mathbb{B}}\newcommand{\bB}{\mathcal{B}}
\newcommand{\Cc}{\mathbb{C}}\newcommand{\cC}{\mathcal{C}}
\newcommand{\Dd}{\mathbb{D}}\newcommand{\dD}{\mathcal{D}}
\newcommand{\Ee}{\mathbb{E}}\newcommand{\eE}{\mathcal{E}}
\newcommand{\Ff}{\mathbb{F}}\newcommand{\fF}{\mathcal{F}}
\newcommand{\Gg}{\mathbb{G}}\newcommand{\gG}{\mathcal{G}}
\newcommand{\Hh}{\mathbb{H}}\newcommand{\hH}{\mathcal{H}}
\newcommand{\Ii}{\mathbb{I}}\newcommand{\iI}{\mathcal{I}}
\newcommand{\Jj}{\mathbb{J}}\newcommand{\jJ}{\mathcal{J}}
\newcommand{\Kk}{\mathbb{K}}\newcommand{\kK}{\mathcal{K}}
\newcommand{\Ll}{\mathbb{L}}\newcommand{\lL}{\mathcal{L}}
\newcommand{\Mm}{\mathbb{M}}\newcommand{\mM}{\mathcal{M}}
\newcommand{\Nn}{\mathbb{N}}\newcommand{\nN}{\mathcal{N}}
\newcommand{\Oo}{\mathbb{O}}\newcommand{\oO}{\mathcal{O}}
\newcommand{\Pp}{\mathbb{P}}\newcommand{\pP}{\mathcal{P}}
\newcommand{\Qq}{\mathbb{Q}}\newcommand{\qQ}{\mathcal{Q}}
\newcommand{\Rr}{\mathbb{R}}\newcommand{\rR}{\mathcal{R}}
\newcommand{\Ss}{\mathbb{S}}\newcommand{\sS}{\mathcal{S}}
\newcommand{\Tt}{\mathbb{T}}\newcommand{\tT}{\mathcal{T}}
\newcommand{\Uu}{\mathbb{U}}\newcommand{\uU}{\mathcal{U}}
\newcommand{\Vv}{\mathbb{V}}\newcommand{\vV}{\mathcal{V}}
\newcommand{\Ww}{\mathbb{W}}\newcommand{\wW}{\mathcal{W}}
\newcommand{\Xx}{\mathbb{X}}\newcommand{\xX}{\mathcal{X}}
\newcommand{\Yy}{\mathbb{Y}}\newcommand{\yY}{\mathcal{Y}}
\newcommand{\Zz}{\mathbb{Z}}\newcommand{\zZ}{\mathcal{Z}}

\newcommand{\sfa}{\mathsf{a}}\newcommand{\fra}{\mathcal{a}}
\newcommand{\sfb}{\mathsf{b}}\newcommand{\frb}{\mathcal{b}}
\newcommand{\sfc}{\mathsf{c}}\newcommand{\frc}{\mathcal{c}}
\newcommand{\sfd}{\mathsf{d}}\newcommand{\frd}{\mathcal{d}}
\newcommand{\sfe}{\mathsf{e}}\newcommand{\fre}{\mathcal{e}}
\newcommand{\sff}{\mathsf{f}}\newcommand{\frf}{\mathcal{f}}
\newcommand{\sfg}{\mathsf{g}}\newcommand{\frg}{\mathcal{g}}
\newcommand{\sfh}{\mathsf{h}}\newcommand{\frh}{\mathcal{h}}
\newcommand{\sfi}{\mathsf{i}}\newcommand{\fri}{\mathcal{i}}
\newcommand{\sfj}{\mathsf{j}}\newcommand{\frj}{\mathcal{j}}
\newcommand{\sfk}{\mathsf{k}}\newcommand{\frk}{\mathcal{k}}
\newcommand{\sfl}{\mathsf{l}}\newcommand{\frl}{\mathcal{l}}
\newcommand{\sfm}{\mathsf{m}}\newcommand{\frm}{\mathcal{m}}
\newcommand{\sfn}{\mathsf{n}}\newcommand{\frn}{\mathcal{n}}
\newcommand{\sfo}{\mathsf{o}}\newcommand{\fro}{\mathcal{o}}
\newcommand{\sfp}{\mathsf{p}}\newcommand{\frp}{\mathcal{p}}
\newcommand{\sfq}{\mathsf{q}}\newcommand{\frq}{\mathcal{q}}
\newcommand{\sfr}{\mathsf{r}}\newcommand{\frr}{\mathcal{r}}
\newcommand{\sfs}{\mathsf{s}}\newcommand{\frs}{\mathcal{s}}
\newcommand{\sft}{\mathsf{t}}\newcommand{\frt}{\mathcal{t}}
\newcommand{\sfu}{\mathsf{u}}\newcommand{\fru}{\mathcal{u}}
\newcommand{\sfv}{\mathsf{v}}\newcommand{\frv}{\mathcal{v}}
\newcommand{\sfw}{\mathsf{w}}\newcommand{\frw}{\mathcal{w}}
\newcommand{\sfx}{\mathsf{x}}\newcommand{\frx}{\mathcal{x}}
\newcommand{\sfy}{\mathsf{y}}\newcommand{\fry}{\mathcal{y}}
\newcommand{\sfz}{\mathsf{z}}\newcommand{\frz}{\mathcal{z}}


      \newcommand{\phdot}{\phantom{.}}

%---------------------  0.2.2. math environments  -----------------------------
\newtheorem*{qst}{Question}
\newtheorem*{thm}{Theorem}
\newtheorem*{lem}{Lemma}
% ...
\theoremstyle{definition}
\newtheorem*{dfn}{Definition}

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  0.3. Section Headers  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


\begin{document}
\samtitle{mlentary (optional 6.86x notes)}

      What tools can we use to automatically extract,
      extrapolate, and explain patterns in data?
      %
      These notes review the main tools we develop in 6.86x.  As binocular
      vision deepens and disambiguates, the differences between how our
      lectures and how these notes organize concepts may also enrich
      understanding.
      \begin{marginfigure}
        \vspace{+0.5cm}
        \includegraphics[width=1.0\textwidth]{necker}
        %\includegraphics[width=0.3\textwidth]{face-vase}
      \end{marginfigure}
      %
      \attn{You do not need to read these notes at all} to get an A
      in this course; conversely, \attn{you may not cite these notes} when
      justifying work in homework or exams.
      %
      %You might enjoy skimming the appendice's three examples. 

  \samsection{prologue} %in three examples}

    %\samsubsection{bird's eye framework}% data flow and goals 
    \samsubsubsection{types of learning}
      How do we communicate
      patterns of desired behavior?
      %new skills?
      %to other humans?  
      %
      %From most to least explicit,
      We can teach:\marginnote[-1cm]{%
          \attn{TODO: five labeled pictures of mushrooms and poison levels
          (say, ``safe'', ``stomach upsetting'', ``very dangerous'').  A sixth,
          un-labeled picture with a question mark for its label. 
          %
          Make explicit that we view mushrooms are prompts/inputs and poison
          levels as answers/outputs.
          }
      }
      \begin{description}
        \item[\textbf{by instruction}:  ]  ``to tell whether a mushroom is poisonous, first look at its gills...'' 
        \item[\textbf{by example}:      ]  ``here are six poisonous fungi; here, six safe ones.  see a pattern?''
        \item[\textbf{by reinforcement}:]  ``eat foraged mushrooms for a month; learn from getting sick.''
      \end{description}
      %
      Machine learning is the art of programming computers to learn from such
      sources.  
      We'll focus on the most important case: learning from examples.\marginnote{%
        $\leftarrow$ We'll see in \S E that learning by example is
        key to the other modes of learning.
      }
      %
      Given a list of $N$ examples of properly answered prompts, we
      seek a pattern: a map from prompts in $\xX$ to answers in $\yY$.\marginnote{%
        $\leftarrow$ Actually, we'll soon account for uncertainty by letting
        patterns map to \emph{probability distributions over} answers; then a
        program that learns from examples has type
        $$
          \lL : (\xX\times \yY)^N \to (\xX\to \text{DistributionsOn}(\yY))
        $$
        %
        Sometimes, the prompt is always the same --- say,
        ``produce a beautiful melody'' --- and we seek (e.g.) to generate
        many answers. % and to learn a complicated distribution over the answers.
        %
        When we thusly focus on the output's structure, we're doing
        \textbf{unsupervised learning}; when our focus is on input-output
        relation, we're doing \textbf{supervised learning}.
        %
        %We won't take this probabilistic view until page 2. 
      }
      %
      %In symbols,
      So a program that learns from examples has type
      $$
        \lL : (\xX\times \yY)^N \to (\xX\to \yY)
      $$
      %

    \samsubsubsection{learning error}
      For $\sS : (\xX\times \yY)^N$ a
      list of examples drawn from nature's distribution $\dD$ on $\xX\times
      \yY$, 
      a pattern $f:\xX\to \yY$
      has a \textbf{training error}
      $
         \Ein(f) = \Pp_{(x,y)\sim \sS}[f(x)\neq y] 
      $,
      an average over examples;
      and a \textbf{testing error}
      $
         \Eout(f) = \Pp_{(x,y)\sim \dD}[f(x)\neq y] 
      $,
      an average over nature.
      %
      We want $\lL$ to map $\sS$ to an $f$ with low $\Eout(f)$.\marginnote{%
        %  TODO: mention extereme class-imbalance and bayesian *decision* theory 
      }
      We often define $\lL$ to roughly minimize $\Ein$ over a
      set $\hH \subseteq (\xX\to \yY)$ of candidates.  Then $\Eout$
      decomposes into the failures
      of $\Ein$ to estimate $\Eout$, % (generalization),
      of $\lL$ to minimize $\Ein$, and % (optimization), and 
      of $\hH$ to contain nature's truth: %(approximation): 
      \newcommand{\minf}[1]{{\inf}_{\hH}}
      \begin{align*}
          \Eout(\lL(\sS)) 
          =~&\Eout(\lL(\sS))      &-\,\,\,&      \Ein(\lL(\sS)) &~\}~& \text{generalization error} \\
          +~&\Ein(\lL(\sS))       &-\,\,\,& \minf{\hH}(\Ein(f)) &~\}~& \text{optimization error} \\
          +~&\minf{\hH}(\Ein(f))  &       &                     &~\}~& \text{approximation error}  
      \end{align*}
      These terms are in tension.  For example, as $\hH$ grows, the
      approximation error may decrease while the generalization error may
      increase (``bias-variance'').

      %In this page's final $2.5$cm we give a full example.
      \begin{marginfigure}
          \par \attn{Six labeled MNIST $(x,y)$ pairs.                          }\vspace{1.5cm}
          \par \attn{
            MENTION $N=24$!
            Left: scatterplot of $N=24$ TRAINING set in width-darkness plane,
              with six above labeled pairs marked and
              with decision line for specific (suboptimal) $(a,b)$ pair drawn and
              with decision line for              optimal  $(a,b)$ pair drawn. 
            Right: grid of all $(a,b)$ pairs, colored by TRAINING error, 
              with same two $(a,b)$ pairs marked.
          }\vspace{1.5cm}
          \par \attn{
            Left: scatterplot of size-96 TESTING set in width-darkness plane,
              with decision line for     training-optimal  $(a,b)$ pair drawn and 
              with decision line for     testing- optimal  $(a,b)$ pair drawn. 
            Right: grid of all $(a,b)$ pairs, colored by TESTING error,
              with same two $(a,b)$ pairs marked. 
          }\vspace{1.5cm}
      \end{marginfigure}
    \samsubsubsection{tiny example}
      $\xX = \{\text{grayscale~}28\!\times\!28\text{-pixel images}\}$; $\yY=\{0,1\}$. %,\cdots,9\}$.
      %Samples %$(x,y)$ from
      $\dD$'s samples are photos $x$ of handwriting of digit $y$. 
      %To sample from $\dD$, we ask a human to write digit $y\in \yY$, ask a human to write
      %that digit, and then photograph their writing.
      %
      Each $x$ has a
      width (number of inked columns)
      and
      darkness (average ink per column).
      %
      $\hH$ contains all $f_{a,b}$ %:\xX\to\yY$ defined by
      defined by:
      $$
        f_{a,b}(x) = ~0 \text{~~if~~} a\cdot \text{width}(x) + b\cdot\text{darkness}(x) < 0 \text{~~else~~} 1 
      $$ 
      Our program $\lL$ loops over all integer pairs $(a,b)$ in $[-99,+99]$
      to minimize $\Ein$.
      %
      %
      $\lL$ gives $(a,b)=(??,??)$; $\Ein = ??\%$ but $\Eout = ??\%$, so we see
      gen.\ error.  Our brute-force loop sets opt.\ error to $\approx 0$.
      Width and darkness lose important information; and straight lines badly
      approximate the boundary between digits; hence our large approx.\ error
      ($\approx ??\%$). 
      %
      In 6.86x we'll address all three issues.

      %from prompts or scenarios
      %to proper answers or actions;
      %%For example, a compiler is a program that sends source
      %%code text to .
      %%A degenerate case  
      %producing in a flash,  
      %Thinking more 
      %We frame learning from examples as 
      %

      %Let us explicitly frame a simple case of the problem.
      %Say that $\Xx$ is a space of images, $\{\pm 1\} =
      %\{\text{Cow}, \text{Dog}\}$ is a set of labels, and we seek a
      %classifier $f: \Xx\to\{\pm 1\}$ that accords with nature.  More
      %precisely, we model nature as a probability distribution $\Dd$ over the
      %space $\{\pm 1\}\times\Xx$ of labeled images and we fix a set $\Hh
      %\subseteq \{\pm 1\}^\Xx$ of candidate classifiers.  With $\Ss \sim
      %\Dd^N$ denoting $N$ samples drawn i.i.d.\ from $\Dd$, the
      %\textbf{training error} of $f\in \Hh$ averages over the empirical
      %distribution $\Ss$:
      %
      %
      %$$
      %    \Ein(f) = \Pp_{(y,x)\sim \Ss}[f(x)\neq y] 
      %$$
      %and the \textbf{testing error} averages over $\Dd$:
      %$$
      %    \Eout(f) = \Pp_{(y,x)\sim \Dd}[f(x)\neq y] 
      %$$

  \newpage
  \samsection{linear models}
    \samsubsection{linear approximations} 
    \samsubsection{iterative optimization} 
    \samsubsection{generalization bounds} 
    \samsubsection{model selection} 
    \samsubsection{priors and generalization} 
    \samsubsection{continuous optimization} 
    \samsubsection{kernels enrich approximations} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \newpage
  \samsection{nonlinearities}
    \samsubsection{fixed featurization} 
    \samsubsection{learned featurizations} 
    \samsubsection{differentiation} 
    \samsubsection{architecture and symmetry} 
    \samsubsection{feature hierarchies} 
    \samsubsection{stochastic gradient descent} 
    \samsubsection{loss landscape shape} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \newpage
  \samsection{structured inference}
    \samsubsection{graphical generative models} 
    \samsubsection{inferring conditional marginals} 
    \samsubsection{learning parameters} 
    \samsubsection{hierarchy and mixtures} 
    \samsubsection{hierarchy and transfer} 
    \samsubsection{variational and sampling methods} 
    \samsubsection{amortized inference} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \newpage
  \samsection{reductions to supervision}
    \samsubsection{beyond the i.i.d.\ hypothesis}
    \samsubsection{to build a tool, use it} 
    \samsubsection{distributions as maps} 
    \samsubsection{self-supervised: downstream tasks} 
    \samsubsection{self-supervised: autoregression} 
    \samsubsection{reinforcement: $q$-values} 
    \samsubsection{reinforcement: exploration-exploitation} 
      \samsubsubsection{out-of-distribution tests}
      \samsubsubsection{dependent samples}
      \samsubsubsection{causality}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \newpage
  \samsection{three brief example projects}
      Now comes a hurried tour of the sorts of analysis these notes discuss.  I
      recommend that you \attn{skim} these three example projects without
      trying to understand each step.

    \samsubsection{flooding plains}

      \samsubsubsection{visualization  } % featurization
      \samsubsubsection{fitting        } % featurization
      \samsubsubsection{model selection} % featurization
      \samsubsubsection{prediction}
      \samsubsubsection{overfitting}

      \samsubsubsection{a whimsical story about automation}
        We automate when we re-cast the previous generation's \emph{methods} as
        mere \emph{data} to produce and manipulate. 

        \begin{marginfigure}
            \includegraphics[width=1.0\textwidth]{seven-days}
            \caption{%
              Machine learning continues our human tradition toward the systematic and automatic.
              %
              What for one generation is a luckily discovered idea or recipe
              (bottom tip of red), the next generation refines by
              careful human thought (green). 
              %
              What to one generation is a task (green) requiring human thought
              is to a future generation merely a routine parameterized by some
              newly discovered recipe (bottom tip of blue).
              %
              And the cycle repeats.
              %
              %%\textbf{Left} column: assumed given.
              %%\textbf{Middle} column: activities in the past done by humans and in the future done by machine.
              %%\textbf{Right} column: artifacts valuable either directly (top right corner) or because they help (via diagonal arrows) achieve more direct goals. 
            }
        \end{marginfigure}

        We once sought safety from floods.  Long ago, if we lived near safe
        riverbanks it was by luck; we then found we could make unsafe rivers safe
        by erecting floodwalls.  This was dangerous work, so we built brick
        laying robots to do the hard part.  The machines needed as input some
        flood-risk estimates: the location and mud-softnesses of the river banks to
        work on.

        So we sought flood-risk estimates.  At first, if our city's flood-risk
        estimates were correct it was by luck; we then found we could translate
        topographic maps to flood-risk estimates by applying geology ideas.  This
        was tedious work, so we built computers to do the hard part.  The
        computers needed as input a risk-estimation \emph{algorithm}: a recipe of
        the geology rules to calculate through.

        We thus sought risk-estimation algorithms.  At first, if our research
        center's risk-estimation algorithms were robust to new landscapes it
        was by luck; we then found we could calibrate our algorithms to
        historical flood data from many cities.  To find the better of two
        candidate algorithms, we had to aggregate all their many small success
        and errors.  This was subtle work, so we developed statistical theories
        to do the hard part.  The theories needed as input a machine learning
        model: a computationally tractable class of candidate algorithms.

        We thus sought machine learning models.  At first, if our machine
        learning models were highly predictive it was by luck; we then found
        general principles to guide model design and selection from domain
        knowledge. % what will we continue to vreate?

        It is these principles that we study in 6.86x.

        Looking back, these are principles that produce machine learning models
        that produce risk-estimation algorithms that produce flood-risk estimates
        that guide the building of floodwalls. 

    \samsubsection{ancient tablets}
      \samsubsubsection{visualization}
      \samsubsubsection{transcription}
      \samsubsubsection{cleaning and imputation}
      \samsubsubsection{modeling and fitting}
      \samsubsubsection{generation}
      \samsubsubsection{a whimsical story about modeling}

    \samsubsection{pairing flavors}
      \samsubsubsection{visualization}
      \samsubsubsection{modeling and fitting}
      \samsubsubsection{regularization}
      \samsubsubsection{explanation}
      \samsubsubsection{domain adaptation}
      \samsubsubsection{a whimsical story about induction}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \newpage
  \samsection{appendices}
    Here are ``refreshers'' on programming and on math, especially on the
    math of high dimensions and of bayes' law.
    They can help jog your memory of the math and
    programming preparation you should have done in preparation for
    6.86x.\marginnote{%
      Depending on your background and personality, you might recognize and
      care that throughout these notes we are a bit sloppy with our probability
      and programming.  No standard compiler can compile our pseudocode;
      likewise, our theorems are actually theorem sketches in need of
      additional analytic hypotheses. 
      %
      For readers so concerned, we assume an additional prerequisite: enough
      technical background that we may leave implicit both the articulation of
      measure-theory technicalities and the implementation of pseudocode.
    }
    They also help establish our naming conventions.  The refreshers are meant
    neither to explain these topics nor to provide a complete outline of
    prerequisites.

    \samsubsection{python programming refresher}
      \samquote{
          \begin{flushleft}
          \texttt{\phantom{}def get\_random\_number():} \\
          \texttt{\phantom{....}return 4 \# chosen by a fair dice roll} \\
          \texttt{\phantom{.............}\# guaranteed to be random}
          \end{flushleft}\phantom{.}
      }{xkcd, Randall Munroe, translated to Python}

      \samsubsubsection{setup}
      \samsubsubsection{state and control flow}
      \samsubsubsection{input/output}
      \samsubsubsection{numpy}

    \samsubsection{math notation and refresher}
      \samquote{
        He had bought a large map representing the sea, \\
        Without the least vestige of land: \\
        And the crew were much pleased when they found it to be \\
        A map they could all understand.
      }{The Hunting of the Snark, Charles Dodgson}

      \samsubsubsection{probability}
        We've tried to use 
          \vspace{-0.10cm}
        $$
          \text{\textsf{sans serif} for the names of random variables,}
          \vspace{-0.15cm}
        $$
        $$
          \text{\emph{italics} for the values they may take, and}
        $$
        $$
          \text{$\mathcal{CURLY~CAPS}$ for sets of such values.} 
        $$
        For example, we write $p_{\sfy|\sfh}(y|h)$ for the probability that the
        random variable $\sfy$ takes the value $y$ conditioned on the event that
        the random variable $\sfh$ takes the value $h$.
        %
        Likewise, our notation $p_{\hat \sfh|\sfh}(h|h)$ indicates the
        probability that the random variables $\hat \sfh$ and $\sfh$ agree in
        value given that $\sfh$ takes a value $h\in \hH$.
      \samsubsubsection{linear algebra}

      \samsubsubsection{calculus}
      \samsubsubsection{higher dimensions}

    \samsubsection{high dimensional geometry}
      \samquote{
        Can I just say Chris for one moment that I have a new theory about the
        brontosaurus. ...  This theory goes as follows and begins now. All
        brontosauruses are thin at one end, much, much thicker in the middle
        and then thin again at the far end. That is my theory, it is mine, and
        belongs to me and I own it, and what it is too.
      }{Flying Circus, Monty Python}

      \samsubsubsection{concentration of measure}

        \begin{lem}[Chernoff]
            The fraction of heads among $N$ i.i.d.\ flips of a biased coin
            exceeds its mean $p$ by $g$ with chance at most 
            $\exp(-Ng^2)$, for $0 \leq p < p+g \leq 1$.
        \end{lem}
        \vspace{-0.5cm}
        \begin{figure}[h]
            \centering
            \includegraphics[height=4cm, clip]{chernoff}
            \caption{{
                We sample points uniformly at random on $N$
                sticks, each with three parts: \textbf{green}
                with length $1-p$, \textbf{red} with length $p$, and
                \textbf{blue} with length $g$.  We call non-blue points
                \textbf{boxed} and non-green points \textbf{hollow}.
            }}
            \label{fig:chernoff}
        \end{figure}
        \vspace{-0.5cm}
        \begin{proof} \renewcommand{\qedsymbol}{}
            Let our coin flips arise from sampling points on sticks
            (Figure \ref{fig:chernoff}), where green means tails, red
            means heads, and we condition on the event that blues do not occur.
            %
            To show that less than $(p+g)N = p^\prime N$ flips are
            heads is to show --- given that all points are \textbf{boxed} ---
            that less than $p^\prime N$ points are red. 
            %
            For any $M$:
            {%
            \begin{align*}
                    & ~ \Pp[\text{$M$ are red $\mid$ all are boxed}] \\
                  = & ~ \frac{\Pp[\text{all hollows are red $\mid$ $M$ hollow}] \cdot \Pp[\text{$M$ are hollow}]}{\Pp[\text{all are boxed}] } \\
                  = & ~ (1 - g/p^\prime)^{M} \cdot (1+g)^{N} \cdot \Pp[\text{$M$ are hollow}]
            \end{align*}
            }%
            We sum over $M\geq p^\prime N$, bound $\Pp[\cdots p^\prime N \cdots] \leq 1$,
            then invoke $(x \mapsto x^{p^\prime})$'s concavity and
            $\exp$'s convexity:
            \begin{align*}
                &~\Pp[\text{at least $p^\prime N$ are red $\mid$ all are boxed}]
                \\ \leq
                &~(1 - g/p^\prime)^{p^\prime N} \cdot (1+g)^{N} \cdot \Pp[\text{at least $p^\prime N$ are hollow}]
                \\ \leq
                &~(1 - g)^N \cdot (1 + g)^{N}
                =
                (1 - g^2)^N
                \leq
                \exp(- Ng^2)
                ~~~~~\square
            \end{align*}
        \end{proof}
        \vspace{-0.5cm}
        The Chernoff bound gives us the control over tails we would expect from the
        Central Limit Theorem, but for finite instead of asymptotically large
        $N$.  In particular, when we learn from much but finite data, the
        training error will \textbf{concentrate} near the testing error.

        Indeed, for any $f\in \Hh$, $\Ein(f)$ is the average of $N$ independent 
        Bernoullis of mean $\Eout(f)$.  So for finite $\Hh$, the gap is
        probably small:
        \begin{align*}
            &~\Pp_{\Ss\sim \Dd^N}[\Egap(\Ll) \geq g] \\
            \leq 
            &~\sum_{f\in \Hh} \Pp_{\Ss\sim \Dd^N}[\Eout(f) \geq \Ein(f) + g] \\
            \leq
            &~|\Hh| \cdot \exp(-Ng^2)
        \end{align*}

        For example, if $\Hh$ is parameterized by $P$ numbers, each represented
        on a computer by $32$ bits, then $|\Hh|\leq 2^{32 P}$ and, with
        probability $1-\delta$, the gap is less than
        $$
            \sqrt{(\log(1/\delta) + 32 P)/N}
        $$
        This bound's sensitivity to the description length $32 P$ may seem
        artificial.  Indeed, the various $\Hh$ used in practice --- e.g.
        linear models or neural networks --- depend smoothly on their
        parameters, so the parameters' least significant bits barely affect 
        the classifier.  In other words, $\Hh$'s cardinality is not an apt
        measure of its size.  The VC-dimension measures $\Hh$ more subtly.


      \samsubsubsection{quadratic forms}
      \samsubsubsection{covariance, correlation, least squares regression}

    \samsubsection{bayesian inference}
      \samquote{So little of what could happen does happen.}{salvador dali}

      \samsubsubsection{conceptual framework}
      We're confronted with an observation or dataset $\sfo$ that comes from
      some unknown underlying pattern $\sfh$.  We know how each possible value
      $h$ for $\sfh$ induces a distribution on $\sfo$ and we have a prior sense
      of which $h$s are probable.  Bayes' law helps us update this sense to
      account for the dataset by relating two functions of $h$:
      $$
        \underbrace{p_{\sfh|\sfo}(h|o)}_{\text{posterior}}
        \propto
        \underbrace{p_{\sfo|\sfh}(o|h)}_{\text{likelihood}}
        \cdot
        \underbrace{p_{\sfh}(h)}_{\text{prior}}
      $$

      Bayes' law underlies most of our analyses throughout these
      notes.\marginnote{%
        Like Newton's $F=ma$, Bayes is by itself inert: to make predictions
        we'd have to specify our situation's forces or likelihoods.  Continuing
        the metaphor, we will rarely solve our equations exactly; we'll instead
        make approximations good enough to build bridges and swingsets.  Still,
        no one denies that $F=ma$ orients us usefully in the world of physics.
        So it is with the law of Bayes.
      }

      Formally, we posit a set $\hH$ of \emph{hypotheses}, a set $\oO$ of
      possible \emph{observations}, and a set $\aA$ of permitted
      \emph{actions}.  We assume as given a joint probability measure
      $p_{\sfo,\sfh}$ on $\oO\times \hH$ and a \emph{cost function}
      $c:\aA\times \hH \to \Rr$.
      %
      That cost function says how much it hurts to take the action $a\in \aA$
      when the truth is $h\in \hH$.
      %
      Our primary aim is to construct a map $\pi:\oO\to \aA$ that makes the
      expected cost $\Ee_{\sfh,\sfo} \, c(\pi(\sfa); \sfh)$ small.

      Below are three examples.  In each case, we're designing a robotic vacuum
      cleaner: $\hH$ contains possible floor plans; $\oO$, 
      possible readings from the robot's sensors.  The examples differ in how
      they define and interpret $\aA$ and $c$.

      \textbf{A}.  $\aA$ consists of probability distributions over $\hH$. We
      regard $\pi(o)$ as giving a posterior distribution on $\hH$ upon
      observation $o$.  Our cost $c(a;h)$ measures the surprise of someone who
      believes $a$ upon learning that $h$ is true. 
      %
      Such \emph{inference problems}, being in a precise sense universal, pose
      huge computational difficulties; we thus often collapse distributions to
      points, giving rise to the distinctive challenge of balancing estimation
      error with structural error.

      \textbf{B}.
      $\aA$ consists of latitude-longitude pairs, interpreted as a guessed
      location of the robot's charging station.  The cost $c(a;h)$ measures how
      distant our guess is from the truth.  
      %
      Such \emph{estimation problems} abound in science and engineering; they
      pose the distinctive challenge of balancing
      sensitivity-to-misleading-outliers against
      sensitivity-to-informative-datapoints.
      %robustness to measurement noise with
      %centeredness. 

      \textbf{C}.  $\aA$ consists of instructions we may send to the motors,
      instructions that induce motion through our partially-known room.  The
      cost $c(a;h)$ incentivizes motion into dusty spaces and penalizes bumping
      into walls.
      %
      We often compose such \emph{decision problems} sequentially; this gives
      rise to the distinctive challenge of balancing exploration with
      exploitation.

      \samsubsubsection{effect of prior choice}
      \samsubsubsection{mixture priors and hierarchy}

      \samsubsubsection{frequentism and choice of prior}
      %\samquote{
      %  I am wiser [than he] ... for
      %  ... he fancies he knows something 
      %  ... whereas I ... do not fancy I do.
      %}{socrates}
      %\samsubsubsection{uniform priors} % jeffries
        Our engineering culture prizes not just \emph{utility} but also
        \emph{confidence}, since strong guarantees on our designs allow 
        composition of our work into larger systems: equality, unlike
        similarity, is transitive.  For example, we'd often prefer a 99\%
        guarantee of adequate performance over a 90\% guarantee of ideal
        performance.  This asymmetry explains our pessimistic obsession with
        worst-case bounds over best-case bounds, cost functions over fitness
        functions, and simple models with moderate-but-estimatable errors over
        rich models with unknownable-but-often-small errors.

        The \emph{frequentist} or \emph{distribution-free} style of statistics
        continues this risk-averse tradition.  In the fullest instance of this
        style, we do inference as if the true unknown prior on $\hH$ is chosen
        adversarially.
        %
        That is, we try to find $\pi$ that makes the following error small:
        $$
          \max_{p_{\sfh}}
          \,
          \Ee_{\sfh \sim p_{\sfh}(\cdot)} \Ee_{\sfo \sim p_{\sfo}(\cdot|\sfh)}
          \,
          c(\pi(\sfo); \sfh) 
        $$
        %
        %For example, suppose $\aA=\hH$ and $c(a;h) = [\![a\neq h]\!]$ is the
        %zero-one cost.
        %The prior in the minimax pair $(\pi, p_{\sfh})$ tends to be pretty
        %uniform. 
        Intuitively, 

        %minimax 
        %``uniform'' prior
        %on hypothesis space.

      \samsubsubsection{p-hacking} % likelihoods, confidence intervals 
      \samsubsubsection{hidden assumptions} % distribution-existence as coherence condition

      \samsubsubsection{(multiple) hypothesis testing}
      %\samquote{
      %  The theory of probabilities is at bottom nothing but common sense
      %  reduced to calculus; it enables us to appreciate with exactness
      %  [what we] feel with a sort of instinct ...
      %}{pierre simon laplace}

      Let's now consider the case where $\hH$ is a small and finite.  We

  \newpage
  \marginnote[-0.05cm]{%
    \textsc{Table of Contents}
    \begin{description}
      %\item[] \vphantom{.} 
      %  \begin{description}
      %  \end{description}
      \item[A. prologue] \phdot 
        %\begin{description}
        %  \item[bird's eye framework] \phdot
        %\end{description}
      \item[B. linear classification] \phdot
        \begin{description}
          \item[linear approximations] \phdot % hypothesis class; briefly mention regression
          \item[iterative optimization] \phdot % perceptron, logistic
          \item[generalization bounds] \phdot % generalization bounds
          \item[model selection] \phdot % validation, featurization from domain knowledge, which objective func?
          \item[priors and generalization] \phdot % priors and regularization
          \item[continuous optimization] \phdot % convex optimization; implicit regularization
          \item[kernels enrich approximations] \phdot
        \end{description}
      \item[C. nonlinearities] \phdot
        \begin{description}
          \item[fixed featurization] \phdot % classic nonlinearities: discrete-continuous (softmax; embedding); normalization; binning 
          \item[learned featurizations] \phdot
          \item[differentiation] \phdot % smoothness assumption, backprop as dynaprog, autodiff
          \item[architecture and symmetry] \phdot % CNNs, RNNs
          \item[feature hierarchies] \phdot % 
          \item[stochastic gradient descent] \phdot % learning rates as riemannian metrics; conditioning; annealing
          \item[loss landscape shape] \phdot % thinking about minima etc
        \end{description}
      \item[D. structured inference] \phdot
        \begin{description}
          \item[graphical generative models] \phdot % directed models
          \item[inferring conditional marginals] \phdot
          \item[learning parameters] \phdot % spectre of marginal likelihood; EM, other variational methods, MCMC
          \item[hierarchy and mixtures] \phdot
          \item[hierarchy and transfer] \phdot
          \item[variational and sampling methods] \phdot % - tie in with deep learning 
          \item[amortized inference] \phdot % - tie in with deep learning 
        \end{description}
      \item[E. reductions to supervision] \phdot
        \begin{description}
          \item[to build a tool, use it] \phdot
          \item[distributions as maps] \phdot
          \item[self-supervised: downstream tasks] \phdot
          \item[self-supervised: autoregression] \phdot
          \item[reinforcement: exploration-exploitation] \phdot
          \item[reinforcement: states and $q$-values] \phdot
              % another paradigm: learning-from-instruction
          \item[beyond i.i.d.] \phdot
        \end{description}
      \item[F. three brief example projects]   \phdot 
        \begin{description}
          \item[example: flooding plains] \phdot
          \item[example: ancient tablets] \phdot
          \item[example: pairing flavors] \phdot
        \end{description}
      \item[G. appendices] \phdot 
        \begin{description}
          \item[python refresher] \phdot
          \item[math refresher] \phdot
          \item[notes on high dimensions] \phdot
          \item[notes on bayes' law] \phdot
              %
        \end{description}
    \end{description}
  }



\end{document}

