\documentclass{article}
\usepackage{amsmath, amssymb, amsthm, tikz-cd}

\newcommand{\Th}[1]{\Theta^{(#1)}}
\newcommand{\wrap}[1]{\left(#1\right)}
\newcommand{\RR}{{\mathbb R}}
\newcommand{\EE}{{\mathbb E}}

\begin{document}
    \begin{center}
        \Large Jesse's $\delta$s\\
        \large A Local Cow --- 2021-02-01
    \end{center}

    Here is an appropriate map from syntax to semantics for Jesse's
    variational-inference-through-integrals project.  The hope is for automatic
    differentiation to be correct, i.e.\ for the following cube to commute: 
    $$\begin{tikzcd}
                                                               & {\rm Syntax} \arrow[ddd, "D_t"', dotted] \arrow[rrr, "{\rm eval}", dotted] &  &                                        & {\rm Semantics} \arrow[ddd, "D_t"] \\
    {\rm Syntax} \arrow[ddd, "D_t"'] \arrow[rrr, "{\rm eval}"] \arrow[ru, "\EE_X", dotted] &                                          &  & {\rm Semantics} \arrow[ddd, "D_t", dotted] \arrow[ru, "\EE_X"'] &                      \\
                                                               &                                          &  &                                        &                      \\
                                                               & {\rm Syntax} \arrow[rrr, "{\rm eval}"', dotted]                    &  &                                        & {\rm Semantics}                    \\
    {\rm Syntax} \arrow[rrr, "{\rm eval}"'] \arrow[ru, "\EE_X", dotted]                    &                                          &  & {\rm Semantics} \arrow[ru, "\EE_X"']                    &
    \end{tikzcd}$$
    In what follows, we will name the set of syntax trees $\mathcal S$ and
    the semantic set (into which those trees are evaluated) $\frak D$.
    The diagram makes clear that symbols such as $D_t$ are overloaded: they
    act both on $\mathcal S$ and on $\frak D$.
    %
    In both cases, the intuition is that $D_t$ differentiates with respect to
    $t$ and that $\EE_X$ integrates with respect to some smooth probability
    density $\rho$ on $(x, y, \cdots z)\in \mathbb{R}^d = X$.  For concision, we assume
    that $\{t\} \sqcup \{x,y,\cdots z\}$ partitions the set of used variables,
    that we differentiate always with respect to $t$, and that we integrate
    always over $X$.  
    %
    To focus on applications, we restrict our attention to the non-dotted
    rectangle.  We shall discuss its nodes, edges, and faces, i.e.\ define
    the worlds $\mathcal S$ of syntax and $\frak D$ of semantics, construct the
    maps $D_t$ and $\EE_X$ and $\text{eval}$, and check that the rectangle
    commutes. 
    %
    %We shall also assume that the user seeks first derivatives.  Our framework
    %is beautiful because it permits correct computation of higher derivatives, 
    %but to explain this takes work.
    
\begin{center}\large\textsc{Syntax}\end{center}
    
    \noindent\textbf{Grammar} ---
    Because we focus on the non-dotted rectangle, it suffices to describe the
    syntax of integrands.
    %
    Our integrand language includes arithmetic and (differentiated) step
    functions.  For concreteness, say we have ring operations, real numbers,
    real-type variable names, and for each natural $d$ the $d$th derivative
    $\Th{d}$ of the step function.
    %
    The grammar has base types \textsc{var},
    \textsc{smooth} (for smooth expressions), and \textsc{distr} with
    natural ``unit inclusions'' $\text{\textsc{var}}\hookrightarrow
    \text{\textsc{smooth}}$ and $\text{\textsc{smooth}}\hookrightarrow
    \text{\textsc{distr}}$; we regard the distributions as forming
    an algebra over the ring of smooth expressions, and we overload the
    ring operation symbols accordingly.  Syntactically, $\Th{d}$ has the type
    $\text{\textsc{smooth}}\to \text{\textsc{distr}}$.

    An example smooth expression is $y^2 - t x^3 - t$.  An example distribution
    is:
    $$
        \Th{0}(tx-y)\cdot\Th{0}(1-tx-ty)\cdot\Th{0}(y)\cdot\Th{0}(1-t/2-y)
        \cdot x^2 y
    $$
    Other standards conventions write $[f<g]$ for $\Th{0}(g-f)$ and $\delta$
    for $\Th{1}$.

    \textbf{Algebraic Structure} ---
    Are $(x+y)$ and $(y+x)$ the same syntax trees?  With the structure
    presented so far, the two differ.  However, it is conceptually useful to
    ignore such distinctions.  We thus redefine the set of syntax trees as the
    set of productions of the aforementioned grammar, \emph{mod the axioms for
    real vector spaces}.  This makes the set of syntax trees into a real vector
    space.  Going further, letting $\frak P$ denote the ring of polynomials on
    $t,x,y,\cdots, z$ with real coefficients, we consider the syntax trees
    as forming a $\frak P$-module called $\mathcal S$ 

    \textbf{Differentiation} ---
    The operator $D_t$ transforms syntax trees to
    syntax trees by application of the product rule, of linearity,
    of the relation 
    $$
        D_t[{\mathcal V}] \triangleq 1~\text{if}~{\mathcal V}=t~\text{else}~0
    $$
    for \textsc{var} node $\mathcal V$,
    and of the chain rule for $\Th{d}$ with \textsc{smooth} tree $\mathcal E$:
    $$
        D_t[\Th{d}]({\mathcal E})] \triangleq \Th{d+1}({\mathcal E}) \cdot D_t[{\mathcal E}]  
    $$
    By induction, $D_t$'s action on integrand syntax trees is thus fully
    determined.  Moreover, it is a standard check (in the algebraic theory of
    \emph{derivations}) that this map is well-defined despite the 
    aforementioned module structure. 

\begin{center}\large\textsc{Semantics}\end{center}

    \noindent\textbf{Construction} ---
    How are we to interpret our integrand language?  We are familiar with the
    ring $\frak R$ of smooth functions on variables $t,x,y,\cdots,z$ whose
    every $d$th derivative ($0\leq d$) is polynomially bounded.  The
    interpretation of syntax trees of type \textsc{smooth} into $\frak R$ is
    standard.  We now construct an $\frak R$-module $\frak D$ of
    \emph{distributions} into which we shall interpret syntax trees of type
    \textsc{distr}.

    Intuitively, evaluation sends a \textsc{distr} $\mathcal D$ to
    the integration-over-$X$ operator 
    $$
        \text{eval}({\mathcal D}) = \wrap{g \mapsto \EE_X \mathcal D \cdot g}
    $$
    To make sense of this, we formally define $\frak D$ as the real vector
    space of continuous $\mathbb{R}$-linear functionals on $\frak G \subseteq
    \frak R$.  Here, $\frak G$, an ideal of $\frak R$, contains the smooth
    functions whose every $d$th derivative ($0\leq d$) is sub-gaussian.\footnote{The
    specifics of sub-gaussianity are unimportant: we just want to avoid
    divergent integrals.  We topologize $\frak G$ by insisting that every ball
    $\{g \in G : \sup_X L(g) < r\}$ be open, where $L$ ranges over polynomials
    of $x,y,\cdots,;d_x,d_y,\cdots,d_z$ and $r$ is positive.}

    \textbf{Algebraic Structure} ---
    The relation $(r\cdot d)(f) = d(r\cdot f)$ for $r,d,g \in {\frak
    R},\frak{D},\frak{G}$ furnishes $\frak D$ with the additional structure of
    an $\frak R$-module.  In particular, since $\frak R$ contains the
    polynomials $\frak P$, we may view $\frak{D}$ as a $\frak P$-module. 

    \textbf{Differentiation and Integration} ---
    We define $D_t$'s action on $\frak D$ pointwise:
    $$
        (D_t d)(g) \triangleq \frac{\partial d(g)}{\partial t}
    $$
    We define $\EE_X$'s action $\frak D$ by demanding that
    $(\EE_X d)(g) = d(\rho) \int_X g$.  Alas, we might have
    $\rho \notin \frak G$.  One solution is to use
    an ever-widening gaussian:
    $
        \wrap{\EE_X d}(g) =^?  
        \lim_{v\to\infty} d(\exp(-(x^2+y^2+\cdots+z^2)/v^2))
        \cdot \EE_X g
    $.

\begin{center}\large\textsc{Evaluation}\end{center}

    \textbf{Definition} ---
    We now define evaluation of syntactic integrands, i.e.\ the map
    $\text{eval}:{\mathcal S}\to{\frak D}$.  Up to applications of the $\frak
    P$-module axioms, the general form of a syntax tree is a polynomial of
    $\Th{d}$s with \textsc{smooth} coefficients and arguments.  As we know
    already how to evaluate \textsc{smooth} expressions into $\frak P$ and as
    we insist that evaluation be $\frak P$-linear, it suffices to define
    evaluation on a generic monomial of $\Th{d}$s.

    Well, we define 
    $$
        \text{eval}\wrap{\prod_i \Th{d_i}(f_i)}
        \triangleq
        \wrap{
            g \mapsto
            \lim_{\epsilon\to 0^+}
            \EE_X g \cdot
            \prod_i \varphi^{(d_i)}(f_i/\epsilon) 
        }
    $$
    Here, $\varphi$ is any smooth monotonic function equal to $0$ for inputs
    below $-1$, equal to $1$ for inputs above $+1$, and symmetrical in
    that $\phi(-x)+\phi(+x)=1$.\footnote{
        The specific choice of $\varphi$ doesn't matter; in fact,
        if we seek only first derivatives (so $D_t \EE_X \cdots$ instead of
        $D_t^2 \EE_X \cdots$), $\varphi$ doesn't even have to be smooth: it
        may linearly interpolate between $(-1,0)$ and $(+1,1)$.
        %
        Still, it's important not to mix up $\varphi_\epsilon$ with
        $\Th{0}$: though $\lim_\epsilon \EE_X \varphi(x/\epsilon)^2 g =
        \lim_\epsilon \EE_X \varphi(x/\epsilon) g$ for $g\in \frak G$, we
        have $\wrap{\Th{0}(x)}^2 \neq \Th{0}(x)$ as syntax trees.
    }
    
    \textbf{Examples} ---
    For convenience, we will write $\varphi_\epsilon(\cdot)$ for
    $\varphi(\cdot/\epsilon)$ and $\varphi_0(\cdot)$ for an actual step
    function from $\RR$ to $\RR$.  Then one may check that 
    $$
        \text{eval}\wrap{\Th{0}(f_0)\Th{0}(f_1)\Th{0}(f_d)}
        =
        \wrap{
            g \mapsto
            \EE_X g \cdot \varphi_0(f_0)\varphi_0(f_1)\cdots\varphi_0(f_d)
        }
    $$
    Likewise, for $0\leq d$ one may check that
    $$
        \text{eval}\wrap{\wrap{\Th{0}(f)}^{d+1}\Th{1}(f)} 
        =
        \text{eval}\wrap{\Th{0}(f)} / (d+1)
    $$
    As a final example, for coprime polynomials $f_0, f_1$ one may check that
    $$
        \text{eval}\wrap{\Th{1}(f_0\cdot f_1)\Th{0}(f_0)} 
        =
        \text{eval}\wrap{\Th{1}(f_1)\Th{0}(f_0)} 
        +
        \text{eval}\wrap{\Th{0}(f_0)}/2 
    $$

\begin{center}\large\textsc{Correctness}\end{center}

    \textbf{of Automatic Differentiation} ---
    We check that evaluation and differentiation commute (front square of
    the cubical diagram), i.e.\ that:
    $$
        \text{eval}\circ D_t 
        =
        D_t\circ \text{eval} 
    $$

    Syntax trees are generated by: real linear combinations, \textsc{var}
    nodes, $\Th{d}(\cdot)$, and multiplication.  It thus suffices to check that
    commutativity through each of these inductive relations.

    The check for real linearity follows \emph{a fortiori}:
    $\text{eval}$ is in fact $\frak P$-linear.  The check for \textsc{var}
    nodes follows from Leibniz's rule for indefinite integrals of smooth
    functions.  The check for $\Th{d}(f)$ follows immediately from the
    definitions of $D_t$ and $\text{eval}$.  

    Finally, we check commutativity on products.  It suffices by $D_t$ and
    $\text{eval}$'s real linearity to check this on any real linear basis,
    for instance on the monomial products of $f$s and $\Th{d}(f)$s for
    polynomials $f$.  We finish by unraveling definitions.  Out of laziness, we
    illustrate this only for a specific product; no new ideas are needed for
    the generic case.
    \begin{align*}
        &~(\text{eval}\circ D_t)\wrap{
            \Th{2}(f)
            \cdot
            \Th{3}(h)
        }(g)
        \\
        =
        &~\lim_{\epsilon\to 0^+}
            \EE_X g \cdot
            \varphi_\epsilon^{(3)}(f) f^\prime \varphi_\epsilon^{(3)}(h)
            +
            \varphi_\epsilon^{(2)}(f) \varphi_\epsilon^{(4)}(h) h^\prime
        \\
        =
        &~-\lim_{\epsilon\to 0^+}
            \EE_X g^\prime \cdot
            \Th{2}(f)
            \cdot
            \Th{3}(h)
        \\
        =
        &~(D_t \circ \text{eval})\wrap{
            \Th{2}(f)
            \cdot
            \Th{3}(h)
        }(g)
    \end{align*}
    The middle equality applies integration by parts; the flanking
    ones are definitions.
    
    \textbf{of Switching Limits} ---
    We check that integration and differentiation commute on the subset
    $\text{eval}(\mathcal S) \subseteq \frak D$ (right-hand square of the
    cubical diagram), i.e.:
    $$
        \textstyle
        D_t \circ \EE_X \circ \,\text{eval} 
        =
        \EE_X\circ \,D_t \circ \,\text{eval} 
    $$
    The key idea is that elements $d$ of $\text{eval}(\mathcal S)$ are so
    smooth that all limits of interest will commute.  Indeed, by linearity
    it suffices to consider the case where $d$ is the evaluation of a monomial:
    \begin{align*}
        &~\wrap{D_t \circ \EE_X}(d)(g) 
        \\
        = 
        &~\frac{\partial}{\partial t} \wrap{
            \lim_{v\to\infty} \EE_X \exp(-(x^2+y^2+\cdots+z^2)/v^2) \cdot \prod_i \varphi^{d_i}(f_i)
        } \cdot \EE_X g
        \\
        = 
        &~\wrap{
            \lim_{v\to\infty} \frac{\partial}{\partial t} \EE_X \exp(-(x^2+y^2+\cdots+z^2)/v^2) \cdot \prod_i \varphi^{d_i}(f_i)
        } \cdot \EE_X g
        \\
        =
        &~\wrap{\EE_X \circ D_t}(d)(g) 
    \end{align*}
    The middle equality applies integration by parts; the flanking
    ones are definitions.

\end{document}

