% author:   sam tenka
% create:   2022-04
% change:   2022-05-02

%==============================================================================
%=====  0.  DOCUMENT SETTINGS  ================================================
%==============================================================================

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  0.0. About and Beyond this Exposition  ~~~~~~~~~~~~~~~~~~~~~~~~

%---------------------  0.0.0. page geometry  ---------------------------------
\documentclass[11pt, justified]{tufte-book}
\geometry{
  left           = 1.0in, % left margin
  textwidth      = 5.0in, % main text block
  marginparsep   = 0.2in, % gutter between main text block and margin notes
  marginparwidth = 1.8in,  % width of margin notes
}

%---------------------  0.0.1. math packages  ---------------------------------
\usepackage{amsmath, amssymb, amsthm, mathtools, bm, moresize, relsize}%, euler}

%---------------------  0.0.2. graphics packages  -----------------------------
\usepackage{graphicx, epsdice, xcolor, float, wrapfig, caption, hyperref}

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  0.1. Header Formatting  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

\definecolor{mgrn}{rgb}{0.15, 0.65, 0.05} \newcommand{\grn}{\color{mgrn}}
\definecolor{mred}{rgb}{0.90, 0.05, 0.05} \newcommand{\red}{\color{mred}}
\definecolor{mblu}{rgb}{0.05, 0.35, 0.70} \newcommand{\blu}{\color{mblu}}
\definecolor{mbre}{rgb}{0.30, 0.45, 0.60} \newcommand{\bre}{\color{mbre}}
\definecolor{mgre}{rgb}{0.55, 0.55, 0.50} \newcommand{\gre}{\color{mgre}}

%---------------------  0.1.0. tidbit headers  --------------------------------
\newcommand{\samtitle} [1]{
  \par\noindent{\Huge \sf \blu #1}
  \vspace{0.1cm}
}

\newcommand{\doublerule}{
  \vspace{0.5cm}
  \hrule
  \vspace{0.1cm}
  \hrule
  \vspace{0.1cm}
}

\newcommand{\samquote} [2]{
    \marginnote[-0.2cm]{\begin{flushright}
    \scriptsize
        \gre {\it #1} \\ --- #2
    \end{flushright}}
}

%---------------------  0.1.1. section headers  -------------------------------

\newcommand{\samsection} [1]{
  \vspace{0.2cm}
  \par\noindent{\LARGE \sf \blu #1}
  \vspace{0.1cm}\par
}

\newcommand{\samsubsection}[1]{
  \vspace{0.2cm}
  \par\noindent{\Large \sf \bre #1}
  %\vspace{0.1cm}\par
}

\newcommand{\samsubsubsection}[1]{
   \vspace{0.1cm}
   \par\noindent{\hspace{-2cm}\normalsize \sc \gre #1} ---
}

\newcommand{\note}[1]{{\blu \textsf{#1}}}
\newcommand{\attn}[1]{{\red \textsf{#1}}}
\newcommand{\ques}[1]{{\grn \textsf{Q: #1}}}

%---------------------  0.1.2. clear the bibliography's header  ---------------
\usepackage{etoolbox}
\patchcmd{\thebibliography}{\section*{\refname}}{}{}{}

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  0.2. Math Symbols and Blocks  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

%---------------------  0.2.0. math markers, delimiters, operators  -----------

\newcommand{\pr}{\prime}

\newcommand{\scirc}{\mathrel{\mathsmaller{\mathsmaller{\mathsmaller{\circ}}}}}
\newcommand{\cmop}[2]{{(#1\!\to\!#2)}}

%---------------------  0.2.1. math helpers  ----------------------------------

%---------------------  0.2.2. probability symbols  ---------------------------

\newcommand{\KL}{\text{KL}}
\newcommand{\EN}{\text{H}}
\newcommand{\MI}{\text{MI}}

%---------------------  0.2.3. double-struck and caligraphic letters  ---------

\newcommand{\Aa}{\mathbb{A}}\newcommand{\aA}{\mathcal{A}}
\newcommand{\Bb}{\mathbb{B}}\newcommand{\bB}{\mathcal{B}}
\newcommand{\Cc}{\mathbb{C}}\newcommand{\cC}{\mathcal{C}}
\newcommand{\Dd}{\mathbb{D}}\newcommand{\dD}{\mathcal{D}}
\newcommand{\Ee}{\mathbb{E}}\newcommand{\eE}{\mathcal{E}}
\newcommand{\Ff}{\mathbb{F}}\newcommand{\fF}{\mathcal{F}}
\newcommand{\Gg}{\mathbb{G}}\newcommand{\gG}{\mathcal{G}}
\newcommand{\Hh}{\mathbb{H}}\newcommand{\hH}{\mathcal{H}}
\newcommand{\Ii}{\mathbb{I}}\newcommand{\iI}{\mathcal{I}}
\newcommand{\Jj}{\mathbb{J}}\newcommand{\jJ}{\mathcal{J}}
\newcommand{\Kk}{\mathbb{K}}\newcommand{\kK}{\mathcal{K}}
\newcommand{\Ll}{\mathbb{L}}\newcommand{\lL}{\mathcal{L}}
\newcommand{\Mm}{\mathbb{M}}\newcommand{\mM}{\mathcal{M}}
\newcommand{\Nn}{\mathbb{N}}\newcommand{\nN}{\mathcal{N}}
\newcommand{\Oo}{\mathbb{O}}\newcommand{\oO}{\mathcal{O}}
\newcommand{\Pp}{\mathbb{P}}\newcommand{\pP}{\mathcal{P}}
\newcommand{\Qq}{\mathbb{Q}}\newcommand{\qQ}{\mathcal{Q}}
\newcommand{\Rr}{\mathbb{R}}\newcommand{\rR}{\mathcal{R}}
\newcommand{\Ss}{\mathbb{S}}\newcommand{\sS}{\mathcal{S}}
\newcommand{\Tt}{\mathbb{T}}\newcommand{\tT}{\mathcal{T}}
\newcommand{\Uu}{\mathbb{U}}\newcommand{\uU}{\mathcal{U}}
\newcommand{\Vv}{\mathbb{V}}\newcommand{\vV}{\mathcal{V}}
\newcommand{\Ww}{\mathbb{W}}\newcommand{\wW}{\mathcal{W}}
\newcommand{\Xx}{\mathbb{X}}\newcommand{\xX}{\mathcal{X}}
\newcommand{\Yy}{\mathbb{Y}}\newcommand{\yY}{\mathcal{Y}}
\newcommand{\Zz}{\mathbb{Z}}\newcommand{\zZ}{\mathcal{Z}}

%---------------------  0.2.2. math environments  -----------------------------
\newtheorem*{qst}{Question}
\newtheorem*{thm}{Theorem}
\newtheorem*{lem}{Lemma}
% ...
\theoremstyle{definition}
\newtheorem*{dfn}{Definition}

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%~~~~~~~~~~~~~  0.3. Section Headers  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


\begin{document}
\samtitle{Diagonalizing Dependencies}

  % section titles:
  %     Towards Trivariate Mutual Information
  %     Universal Features
  %     Nearly Markov
  %     Self Supervised Learning

  The essence of deep learning is to implicitly characterize a
  concept-to-be-learned by specifying how it should behave in composition with
  other concepts.  Then to do gradient desceint.  That is, \emph{we build our
  tools by using them}.  This Grothendieckian framework illuminates on many
  global architectural themes of deep learning: autoencoders, cycle consistency
  losses, attention layers, siamese networks, contrastive losses.  A key
  example is self-supervised representation learning (e.g.\ Word2Vec), 
  we learn to represent data by attempting to predict some of its ``parts'' or
  ``aspects'' from others. 

  In these notes we elaborate on this theme in the case of weak dependencies.
  We begin in the first of four sections by understanding dependency structures
  relevant to more than two variables.

  \samsection{Geometry of $n$-ary Mutual Information}

    \samsubsection{Interactions, Distributivity, Local-Global}%
      \samquote{%
         I believe that either Jupiter has life or it doesn't.
         But I neither believe that it does, nor do I believe that it doesn't.
      }{Five Thousand B.C., Raymond Smullyan}

      \samsubsubsection{Inclusion-Exclusion}
        Why do we love entropy $\EN(X)$ and mutual information $\MI(X;Y)$?
        %
        One reason is that they are asymptotically invariant under data
        re-formatting: up to asymptotically negligible error, we can losslessly
        translate a long stream $X=(x_t:0\leq t<T)$ to a stream
        $X^\pr=(x_t^\prime:0\leq t<T)$ when the $\EN(X)\leq \EN(X^\pr)$.
        By \emph{losslessly} I mean that $X^\pr$ should determine $X$.\marginnote{%
          Another operational characterization of mutual information is how
          much information Alice would have to send to Bob if Alice translates
          $X$ to $X^\pr$ and then Bob translates $Y$ to $Y^\pr$ and they want
          to minimize filesize.
        }
        %
        Likewise, we can super-losslesly translate $(X,Y)$ to $(X^\pr,Y^\pr)$  
        when $\EN(X)\leq \EN(X^\pr)$ and $\EN(Y)\leq \EN(Y^\pr)$ \emph{and} $\MI(X;Y)\leq \MI(X^\pr;Y^\pr)$.
        By \emph{super-losslessly} I mean that $X^\pr$ should determine $X$ and
        $Y^\pr$ should determine $Y$.
        %
        These entropic conditions (with equality rather than mere inequality)\marginnote{%
          $\leftarrow$ I think of water sloshing around in a balloon animal.
        }
        are necessary and sufficient (up to negligible error) for
        \emph{bi-directional} super-lossless translation.  We'll call such a
        two-way translation an \emph{isomorphism} between same-sized tuples of
        random variables.  For tuples of size $1$ and $2$, the ``richness''
        functions $\EN(\cdot)$ and $\MI(\cdot;\cdot)$ suffice to characterize
        isomorphism type.


        \ques{What numbers characterize the isomorphism type of a tuple $(X,Y,Z)$?}
        We will see that pairwise mutual information does not suffice and we
        \attn{hypothesize} that even including triplet mutual information (in
        the sense of Ting 1962) does not suffice.

        %   2
        % 2 2 2
        %1 1 1
        %  0

      \samsubsubsection{Venn Diagrams Mislead}
        I contend that $3$-ary mutual information is slippery to define
        to our satisfaction due to a failure of additivity (precisely,
        distributivity) of information.  An analogous situation occurs in the
        linear algebra: Note that if $A,B,C$ are subspaces
        of a finite dimensional vector space, then
        $$
          \dim(A+B) = \dim(A) + \dim(B) - \dim(A \cap B)
        $$
        $$
          \dim(A\cap B) = \dim(A) + \dim(B) - \dim(A+B)
        $$
        These reflect inclusion-exclusion in a venn diagram.  Are the
        following also true?
        \begin{align*}
          \dim(A+B+C) =& \dim(A) + \dim(B) + \dim(C)
                     \\&-\dim(A \cap B) - \dim(B \cap C) - \dim(C \cap A)
                     \\&+\dim(A \cap B \cap C) 
        \end{align*}
        \begin{align*}
          \dim(A\cap B\cap C) =& \dim(A) + \dim(B) + \dim(C)
                             \\&- \dim(A + B) - \dim(B + C) - \dim(C + A)
                             \\&+ \dim(A + B + C) 
        \end{align*}
        No!  That's what we would expect from a Venn diagram, but it is wrong.
        Venn models $\MI$ badly for analogous reasons.  
        I think of $\MI(X;Y;Z)$ not as a mere real number (like the area of a Venn lune)
        but as a geometric structure (much
        as a lattice of subspaces generated by sums and intersections of given
        subspaces carries more data than just the dimensions of the pure sums).

        Here's a concrete linear algebra example.\marginnote{%
            Say $V;V^\pr$ have bases $u,v,x;u^\pr,v^\pr,x^\pr$
            Let $\{u\},\{v\},\{u+v\}$ span $A,B,C$.
            Let $\{u^\pr\},\{v^\pr\},\{x^\pr\}$ span $A^\pr,B^\pr,C^\pr$.
        }
        Let $A,B,C$ be three coplanar, pairwise distinct $1$-D subspaces of a
        three-dimensional space $V$.
        Let $A^\pr,B^\pr,C^\pr$ be three non-coplanar $1$-D subspaces of a
        three-dimensional space $V^\pr$.
        For either triplet, the intersection of any $n$
        of the subspaces has dimension $3,1,0,0$ for $n=0,1,2,3$.  And yet
        the two situations are \emph{not} isomorphic.  So intersection dimensions
        --- even all $2^3$ dimensions together --- do not determine isomorphism
        type.

        Fixing inner products on $V,V^\pr$ and taking orthogonal complements of
        each subspace shows that sum-dimensions do not determine 
        isomorphism type either.

        % THIS PARAGRAPH IS WRONG!  C IS NOT DISTINGUISHED!
        %Another example where the dimension data of all $2^3$ many
        %intersections fails to determine isomorphism type is that $(A,B,C)$
        %have the same intersection dimensions as $(B,C,A)$, even though $C$ is
        %distinguished: $C\subset A+B$ but $A\not\subset B+C$.

      \samsubsubsection{Distributivity}
        Inspecting the above paragraphs' entropy and linear algebra examples,
        we see that key difference from the worlds of measure and
        of cardinality is a failure of distributivity.  In linear algebra,
        even when $A\cap B=0$ we can have 
        $$
            A\cap C + B\cap C \subseteq (A+B)\cap C
        $$
        but often not the other inclusion.  And for $A,B$ independent we have\marginnote{%
          Without independence, it can go the other way: think of bits with
          $A=B=C$.
        }
        $$
            \MI(A \,;\, C) + \MI(B \,;\, C) \leq \MI(A,B \,;\, C) 
        $$
        but often not the other inequality (think of bits with $A+B+C=0$).
        Indeed\marginnote{%
          \attn{Can we use the variational formulation instead?
          We'd take $f_{A;C}, f_{B;C}$ witnesses
          (up to $\epsilon$ error) to $\MI(A;C),
          \MI(B;C)$'s values.  We'd set $f_{A,B;C}((a,b),c)$ to be their sum or something
          to get a lower
          bound on $\MI(A,B;C)$:}
        }:
        \begin{align*}
            [(A)+(B)+(C)-(ABC)]
            &- [(A)+(C)-(AC)]-[(B)+(C)-(BC)]
            \\&=
            (AC)+(BC)-(C)-(ABC)
            \\&=
            [(C|A)+(A)]
            +[(C|B)+(B)]
           -(C)
            -[(C|AB)-(A)-(B)]
            \\&=
            (C|A)+(C|B)-(C|AB)-(C|)
            \\&\geq
            (C|A)+(C|B)-(C|)-(C|)
               \geq 0
        \end{align*}
        Above, $(\cdots|\cdots)$ is shorthand for the conditional entropy
        between the tuples $\cdots$ and $\cdots$ of variables; $(\cdots)=(\cdots|)$, for
        the unconditional entropy.

      \samsubsubsection{Cohomology}
        Cohomology is a scary word for the study of how local data patches
        together to make global data.
        %
        A classical example is that we can make a sphere from two hemispheres,
        either of which doesn't have a ``hole''; but the sphere has a hole.
        %
        We summarize the (\href{https://www.mdpi.com/1099-4300/17/5/3253/pdf?version=1431515279}{2015 theory})
        of Pierre Baudot and Daniel Bennequin. 

    \samsubsection{Information Lattice Geometry}%Intersections, Complements, Cohomology}
      \samquote{%
          One beginning and one ending for a book was a thing I did not agree
          with. A good book may have three openings entirely dissimilar and
          inter-related only in the prescience of the author, or for that
          matter one hundred times as many endings
      }{At Swim-Two-Birds, Brian O'Nolan}

      %\samsubsubsection{Geometry}




  %\note{What is a revealing way to visualize or plot a joint distribution on
  %two nearly independent categorical variables?}
  In what sense is the left figure a better featurization of our data than the right? 

  \samsection{Universal Features}
    \samquote{
      Independence is a heady draft...
    }{Mom \& Me \& Mom, Maya Angelou}

    We summarize the (\href{https://arxiv.org/abs/1911.09105}{2019 theory}) of
    Shao-Lun Huang, Anuran Makur, Gregory W.\ Wornell, and Lizhong Zheng. 

    \samsubsection{MI between nearly joint-uniform variables}

      Suppose $X,Z$ have uniform marginals and are nearly independent --- 
      $$
        p(x^\pr, z^\pr) = \frac{\exp(\epsilon \pi(x^\pr, z^\pr))}{|X\times Z|} 
      $$
      --- in that their mutual information $\MI(X;Z)$ is $\ll \epsilon^2$.
      Here, $\pi(\cdot, \cdot) \ll 1$.
      Write $M=|X\times Z|$.

      \samsubsubsection{Bird's Eye Story}
        We wish to featurize $X,Z$ into
        $k$-dimensional variables $X^\pr, Z^\pr$ so as to preserve as much shared
        information as possible:
        $$
          X^\pr - X - Z - Z^\pr
          \quad\quad
          \MI^\pr(X^\pr;Z^\pr) \lesssim \MI(X;Z)
        $$

        Now, our notion $\MI^\pr$ of shared information ought not literally be
        mutual information; otherwise, for positive $k$ almost all embeddings of
        $X,Z$ into $\Rr^k$ would trivially preserve information. 
        %
        Still, there is a sense in which some embeddings of $X,Z$ are ``more
        aligned'' or ``more co-informative'' than others.  We'll focus on the
        notion of \emph{accessible co-informativity}: that is, on a measure of
        how related variables appear despite bounded computation to compare them.
        %
        Variational bounds provide a convenient way to model bounded computation:
        $$
          \MI(X;Z) = \sup_c\,
          \Ee_{x,z \sim \mu_{X\times Z}} \left[
            c(x,z)
          \right]
          -
          \log
          \Ee_{x,z \sim \mu_{X} \times \mu_{Z}} \left[
          \exp
            c(x,z)
          \right]
        $$
        with the supremum achieved at $c_{x,z}=\log(p_{x,z}/(p_x p_z))=\epsilon \pi_{x,z}$.
        Let's define $\MI^\pr$ to be the same thing except the function $c$ over
        which we vary must be \textbf{bilinear}.
 
        What does that $\MI$ objective look for $c$ near the unconstrained optimum
        $c=\epsilon \pi$?  Well, for $c=\epsilon(\gamma+\pi)$
        we have to second order in $\epsilon$ and terms constant in $\gamma$:
        \begin{align*}
          \cdots
          &=
          \sum_{x,z} \epsilon\gamma_{x,z} \cdot \exp(\epsilon\pi_{x,z})/M
          -
          \log\left(1 + \sum_{x,z} (\exp(\epsilon \gamma_{x,z})-1) \cdot \exp(\epsilon\pi_{x,z})/M\right)
          \\
          &\in
          C-\frac{1}{2}
          \sum_{x,z} (\epsilon\gamma_{x,z})^2 \cdot \exp(\epsilon\pi_{x,z})/M
          +
          \frac{1}{2}
          \left(\sum_{x,z} \epsilon \gamma_{x,z} \cdot \exp(\epsilon\pi_{x,z})/M\right)^2
          + o(\epsilon^2)
          \\
          &=
          C-\frac{\epsilon^2}{2}
            \text{Var}_{x,z\sim \mu_{X\times Z}} [\gamma_{x,z}] + o(\epsilon^2)
        \end{align*}
        Neither the true nor approximate $\MI$ objectives care whether we
        translate $\gamma$ vertically.  So let's take $\gamma$ to have mean zero
        with respect to the true joint.
        %
        The above says that for constrained $c$ (so long as the feasible set
        gets very close to the true optimum), we want a constrained $c$ that
        minimizes a least-squares error 
        $\text{Var}_{x,z\sim \mu_{X\times Z}} [\epsilon\pi - c]$ --- wow!

        Let's consider $c$ bilinear in $x^\pr,z^\pr$ (the latter featurized per $f,g$):
        $$
          c_{x,z} = \epsilon \sum_k f_x^k g_z^k 
        $$
        Momentarily neglecting centering and the difference between variance with
        respect to joint vs independent marginals, we see that the SVD minimizes
        that least-squares error!
        \par\noindent
        \textbf{Desired Result}: \emph{To leading order in $\epsilon$, the
        optimal features $f,g$ are proportional to the top $k$ non-trivial
        singular vectors of $\exp(\epsilon\pi)$ (when SVD and optimization are
        uniquely soluble).} 

      \samsubsubsection{Technical Details}
        %\attn{We should be more careful, so we state
        %the following conjecturally (TODO: also address that perturbative
        %analysis of ill-conditioned and/or constrained optimization must be
        %handled delicately)}
        
        This result justifies the truncation developed in HMWZ 2019 (\S IV.C).

      \samsubsubsection{Comparison to HMWZ}
        That paper characterizes its truncation as a solution to five
        optimization problems (\S III.B; \S V.C thru \S V.F), each of which :
        (\attn{TODO: verify!})
        \begin{table}
          \begin{tabular}{c|cccccc}
                        &\S III.B       &\S V.C         &\S V.D         &\S V.E         &\S V.F         &   ours        \\ \hline
            norm        &   Ky-fan      &               &               &               & nuclear       & information   \\
            constraint  &               &               &               & feature noise &               & computation   \\
                        &   Ky-fan      &               &               &               &               &                 
          \end{tabular} 
        \end{table}
        \begin{itemize}
          \item relies on a norm introduced manually rather than derived from $\KL$ (Ky-fan, nuclear, ) or
          \item supposes, besides near-independence of $X,Z$, an asymptotically extreme feature noise
        \end{itemize}
        By contrast, in our analysis the universal features fall out from a
        linearity constraint (having to do with pure linearity rather than
        norms).

      % TODO: compare to max correlation analysis in section 3
      %max f, g \Ee(f(x) g(y))
      % TODO: what questions does this bounded-computation viewpoint help answer? 
      % --- analyze random features idea?
      % TODO: 
      % 

      %Then, very explicitly, we have
      %\begin{align*}
      %  \MI^\pr
      %  &=
      %  \sup_{f,g}
      %  \epsilon \sum_{x,z} \exp(\epsilon\pi_{x,z}) \sum_k f_x^k g_z^k/|X\times Z|
      %  -
      %  \log(\sum_{x,z} \exp(\epsilon \sum_k f_x^k g_z^k)/|X\times Z|)
      %  \\
      %  &\approx
      %  \sup_{f,g}
      %  \epsilon \sum_{x,z} \exp(\epsilon\pi_{x,z}) \sum_k f_x^k g_z^k/|X\times Z|
      %  -
      %  \log(\sum_{x,z} \exp(\epsilon \sum_k f_x^k g_z^k)/|X\times Z|)
      %\end{align*}
      %(Use compactness+continuity to pull $O(\epsilon^d)$ out of the $\sup$).

    \samsubsection{adaptation to non-uniform marginals}
      \samquote{
        He had bought a large map representing the sea, \\
        Without the least vestige of land: \\
        And the crew were much pleased when they found it to be \\
        A map they could all understand.
      }{The Hunting of the Snark, Charles Dodgson}

    \samsubsection{principal components}
      We review principal component analysis.
      %\samsubsubsection{pythagoreas, grassmann, gauss}
      \samsubsubsection{pythagoreas and gauss} 
      \samsubsubsection{sidetrack: causal grassmanians}
        Pearl's do-calculus --> fit a nilpotent matrix to data?!
        % A--B--C--D--E--F
        PCA:correlation :: ?:causation

        \begin{verbatim}
        (I + T) nxn
        T^n = 0 
         FED
        (1... 
        ( 1..
        (  1.
        (   1)
        \end{verbatim}

        \attn{TODO: in nearly markov case, notion of ``orthogonality'' is
        different... offdiagonals ... perhaps a triangularization?? what does this look like?}

      \samsubsubsection{}
      \samsubsubsection{fischer metric}

    \samsubsection{greg's analysis of weak pairwise dependencies}
    \samsubsection{sample complexity}

  \samsection{Nearly Markov}
    \samquote{
        ... it invariably delivered a cupful of liquid that was almost, but not quite, entirely unlike tea.
    }{Hitchhiker's Guide to the Galaxy, Douglas Adams}

    \begin{verbatim}
            x1'    x2'    y'
             |     |      |
            x1-----x2-----y
                   |
                  x2'
    \end{verbatim}
    We're studying a perturbation of a system exactly described by a markov
    structure (shoe horizontal edges far from nearly-independent).  One
    interesting task in this setting is to predict y based on x2, then
    fine-tune based on x1.  This relates to self-supervised learning for
    representation learning; pretraining.  
    %
    We want to perturb in ``p(y|x2) vs p(y|x2,x1)''; perhaps the right analogue
    to the original story is for us to assume access to two-point but not
    three-point marginals.

    Start with understanding for one simple SVD in nearly markov how to
    interpret that SVD.

    Now, \emph{what are analogues of the original story's singular vectors?}
    %
    The bounded-computation framework suggests that we maximize something like
    $$
        \MI^\pr(x1^\pr;y^\pr \,|\, x2)
        \quad
        \quad
        \text{or maybe}
        \quad
        \quad
        \MI^\pr(x1^\pr;y^\pr \,|\, x2^\pr)
    $$
    The thing to do, though, is to define $\MI^\pr$!
    For instance, might we condition on a ``mediating'' variable just by taking
    an expectation over the $\MI^\pr$s of its conditionals, as follows?
    $$
      \MI^\pr(X;Z\,|\,M) = \Ee_{m\sim \mu_{M}} \sup_{c~\text{bilinear}}\,
      \Ee_{x,z \sim \mu_{X\times Z|M}(\cdot|m)} \left[
        c(x,z)
      \right]
      -
      \log
      \Ee_{x,z \sim \mu_{X|M}(\cdot|m) \times \mu_{Z|M}(\cdot|m)} \left[
      \exp
        c(x,z)
      \right]
    $$
    Or should $c$ instead somehow know about $m$ as well?

    Relatedly, if we condition on $x2^\pr$ then is there a way we would
    like to incorporate $x2^\pr$'s linear structure?
    And if we condition instead on $x2$, is there a decomposition theorem reducing to old work.



  \samsection{Self-Supervised Learning}
    %\samquote{I am so clever that sometimes I don't understand a single word of what I am saying.}{Oscar Wilde}
    %\samquote{Be a pattern to others and then all will go well.}{Marcus Cicero}

    %\samquote{
    %  Possession of anything new or expensive only reflected a person's lack of
    %  theology and geometry; it could even cast doubts upon one's soul.
    %}{A Confederacy of Dunces, John Toole}
    %\samquote{
    %    Meaning is not only about things ... but also involves felt experiencing. ....
    %    We are most aware of this second ... dimension of meaning in those cases where the symbols do not adequately symbolize the meaning we experience.
    %}{Experiencing, Eugene Gendlin}


\end{document}


      %$c_{x,z}-c_{\tilde x, \tilde z} \in O(\epsilon)$ as well.  Say $c_{x,z} =
      %\epsilon \gamma_{x,z}$.  Then to first order in $\epsilon$:
      %\begin{align*}
      %  1 + \epsilon \pi_{x,z}
      %  \approx
      %  (1 + \epsilon \gamma_{x,z})
      %  \frac{|X\times Z|}
      %    {\sum 1 + \epsilon \gamma_{\tilde x, \tilde z}}
      %\end{align*}
      %and $\gamma \approx \pi$ (remember that $\sum \pi = 0$ exactly). 
      %We can expand to second order to see deviations from proportionality:
      %\begin{align*}
      %  \exp(\epsilon \pi_{x,z})
      %  \approx
      %  \exp(\epsilon \gamma_{x,z})
      %  \frac{|X\times Z|}
      %    {\sum 1 + \epsilon \gamma_{\tilde x,\tilde z} + \epsilon^2 \gamma_{\tilde x,\tilde z}^2/2}
      %\end{align*}


%      The $\MI$ objective's partial with respect to $c_{x,z}$ is:
%      \begin{align*}
%        p_{x,z}
%        -
%        p_{x} p_{z} \cdot
%        \frac{\exp c_{x,z}}{\sum p_{\tilde x} p_{\tilde z}
%        \exp c_{\tilde x,\tilde z}}
%        = 0
%      \end{align*}
%      Using $p_{x,z} / \exp(\epsilon \pi_{x,z}) = p_{x} p_{z} = 1/|X\times Z|$, we have
%      \begin{align*}
%        \exp(\epsilon \pi_{x,z})
%        =
%        \exp(\epsilon \gamma_{x,z})
%        \cdot
%        \frac{|X\times Z|}
%          {\sum 1 + \epsilon \gamma_{\tilde x,\tilde z} + \epsilon^2 \gamma_{\tilde x,\tilde z}^2/2 + \cdots}
%      \end{align*}
%      Thus, $\exp(\epsilon \pi_{x,z})$ and $\exp(\epsilon \gamma_{x,z})$
%      are proportional and 
%      {\color{red}
%      (TODO: address that ill-conditioned constrained optimization
%      can be untreatable by perturbation)} 
%      $$
%        \epsilon\gamma = \epsilon\pi +  
%        \log\left(1 + \sum \frac{\exp(\epsilon\gamma_{\tilde x,\tilde z})-1}{|X\times Z|}\right)
%      $$
%      Now, assuming $\gamma - \pi \in O(\epsilon)$, that $\log$ term is
%      $O(\epsilon^2)$ since $\sum \exp(\epsilon\pi)-1 = 0$ exactly.
%      %
%      This justifies a perturbative expansion:
%      $$
%        \gamma = \pi +
%        \epsilon
%        \sum \frac{\exp(\epsilon\pi_{\tilde x,\tilde z})-1}{|X\times Z|} + o(\epsilon) 
%      $$
%
%
%      Now, the objective's partial with respect to $f_x^k$ is:
%      $$
%        \epsilon \lambda_k = 
%        \frac{\epsilon \sum_{\tilde z} g_{\tilde z}^k \exp(\epsilon \pi_{x,\tilde z})}
%        {\sum_{\tilde x, \tilde z} 1} 
%        -
%        \frac{\epsilon \sum_{\tilde z} g_{\tilde z}^k \exp\left(\epsilon \sum_{\tilde k} f_{x}^{\tilde k} g_{\tilde z}^{\tilde k}\right)}
%        {\sum_{\tilde x, \tilde z} \exp\left(\epsilon \sum_{\tilde k} f_{\tilde x}^{\tilde k} g_{\tilde z}^{\tilde k}\right)}
%      $$
%      We introduced a lagrange multiplier since
%      we insist on the constraint that $\sum_x f_x^k = 0 = \sum_z g_z^k$,
%      i.e., that the embeddings are centered.  This makes both denominators 
%      above $1+O(\epsilon^2)$.  To leading order:
%      $$
%        \lambda_k = \epsilon \sum_{\tilde z} g_{\tilde z}^k (\pi_{x,\tilde z} - \sum_{\tilde k} f_{x}^{\tilde k} g_{\tilde z}^{\tilde k})
%      $$
%      We SUPPOSE THAT $\lambda=0$; then for each $x$, $g^k$ is orthogonal
%      to the residual $\pi - c$.
%      {\color{red}
%
%
